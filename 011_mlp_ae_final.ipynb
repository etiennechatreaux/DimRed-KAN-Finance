{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "450b2e4e",
   "metadata": {},
   "source": [
    "# MLP Autoencoder - Grille d'Hyperparam√®tres Compl√®te\n",
    "Entra√Ænement syst√©matique sur tous les datasets avec hyperparam√®tres optimaux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fe298c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Device: cuda\n",
      "   GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "   Memory: 8.0 GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from src.models.ae_mlp import MLPAutoencoder\n",
    "from src.utils.new_preprocessing import preprocessing_dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.set_float32_matmul_precision('medium')  # Performance boost\n",
    "\n",
    "# Configuration device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üöÄ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597910b7",
   "metadata": {},
   "source": [
    "## 1. Grille d'Hyperparam√®tres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6261d34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total combinations: 13,824\n",
      "‚è±Ô∏è  Estimated time (5min/model): 1152.0 hours\n",
      "üéØ Sampling to 20 combinations per dataset\n"
     ]
    }
   ],
   "source": [
    "# Grille d'hyperparam√®tres extensive mais optimis√©e\n",
    "HYPERPARAMS_GRID = {\n",
    "    # Architecture\n",
    "    'hidden_layers': [\n",
    "        (64, 32),      # Small\n",
    "        (128, 64),     # Medium  \n",
    "        (256, 128),    # Large\n",
    "        (128, 64, 32), # Deep medium\n",
    "    ],\n",
    "    'latent_dims': [3, 5, 8, 32],\n",
    "    \n",
    "    # Regularization\n",
    "    'dropout_rates': [0.0, 0.2, 0.3],\n",
    "    'use_batch_norm': [False, True],\n",
    "    \n",
    "    # Optimization\n",
    "    'learning_rates': [1e-4, 5e-4, 1e-3, 2e-3],\n",
    "    'weight_decays': [0.0, 1e-6, 1e-5],\n",
    "    'batch_sizes': [64, 128, 256],\n",
    "    \n",
    "    # Training\n",
    "    'activations': ['relu', 'silu'],\n",
    "    'loss_types': ['mse', 'huber'],\n",
    "    'epochs': [100],  # Fixed pour temps raisonnable\n",
    "    'patience': [10]\n",
    "}\n",
    "\n",
    "# Estimation du nombre total de combinaisons\n",
    "total_combinations = np.prod([len(v) for v in HYPERPARAMS_GRID.values()])\n",
    "print(f\"üìä Total combinations: {total_combinations:,}\")\n",
    "print(f\"‚è±Ô∏è  Estimated time (5min/model): {total_combinations * 5 / 60:.1f} hours\")\n",
    "\n",
    "# Configuration pour √©chantillonnage intelligent\n",
    "MAX_COMBINATIONS_PER_DATASET = 20\n",
    "print(f\"üéØ Sampling to {MAX_COMBINATIONS_PER_DATASET} combinations per dataset\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e8767",
   "metadata": {},
   "source": [
    "## 2. Fonctions Utilitaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a27d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_directory():\n",
    "    \"\"\"Cr√©e le r√©pertoire de r√©sultats avec timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = Path(f\"results/mlp_ae_grid_search_{timestamp}\")\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return results_dir\n",
    "\n",
    "def sample_hyperparams(grid, n_samples=200, seed=42):\n",
    "    \"\"\"√âchantillonnage intelligent des hyperparam√®tres\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # G√©n√©ration de toutes les combinaisons\n",
    "    keys = list(grid.keys())\n",
    "    values = list(grid.values())\n",
    "    all_combinations = list(itertools.product(*values))\n",
    "    \n",
    "    if len(all_combinations) <= n_samples:\n",
    "        selected_combinations = all_combinations\n",
    "    else:\n",
    "        # √âchantillonnage stratifi√© pour couvrir l'espace\n",
    "        selected_combinations = np.random.choice(\n",
    "            len(all_combinations), \n",
    "            size=n_samples, \n",
    "            replace=False\n",
    "        )\n",
    "        selected_combinations = [all_combinations[i] for i in selected_combinations]\n",
    "    \n",
    "    # Conversion en liste de dictionnaires\n",
    "    sampled_configs = []\n",
    "    for combo in selected_combinations:\n",
    "        config = dict(zip(keys, combo))\n",
    "        sampled_configs.append(config)\n",
    "    \n",
    "    return sampled_configs\n",
    "\n",
    "def evaluate_model_performance(model, X_test, scaler=None):\n",
    "    \"\"\"√âvaluation rapide des performances\"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "        X_recon, latent = model(X_test_tensor)\n",
    "        \n",
    "        # M√©triques de reconstruction\n",
    "        mse = torch.nn.functional.mse_loss(X_recon, X_test_tensor).item()\n",
    "        mae = torch.nn.functional.l1_loss(X_recon, X_test_tensor).item()\n",
    "        \n",
    "        # Variance expliqu√©e approximative\n",
    "        total_var = torch.var(X_test_tensor).item()\n",
    "        residual_var = torch.var(X_test_tensor - X_recon).item()\n",
    "        explained_var = max(0, 1 - residual_var / total_var)\n",
    "        \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'explained_variance': explained_var,\n",
    "        'latent_std': torch.std(latent).item()\n",
    "    }\n",
    "\n",
    "def save_results(results, results_dir, dataset_name):\n",
    "    \"\"\"Sauvegarde des r√©sultats\"\"\"\n",
    "    # JSON pour les m√©tadonn√©es\n",
    "    json_file = results_dir / f\"{dataset_name}_results.json\"\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # CSV pour analyse facile\n",
    "    df_results = pd.DataFrame(results)\n",
    "    csv_file = results_dir / f\"{dataset_name}_results.csv\"\n",
    "    df_results.to_csv(csv_file, index=False)\n",
    "    \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad80da8",
   "metadata": {},
   "source": [
    "## 3. Chargement et Pr√©traitement des Donn√©es\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70863a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading datasets...\n",
      "‚úÖ S&P 500 Full (3734 days, 423 stocks)\n",
      "‚úÖ Information Technology (3734 days, 53 stocks)\n",
      "‚úÖ Financials (3734 days, 64 stocks)\n",
      "‚úÖ Health Care (3734 days, 53 stocks)\n",
      "‚úÖ Industrials (3734 days, 64 stocks)\n",
      "‚úÖ Consumer Discretionary (3734 days, 43 stocks)\n",
      "‚úÖ US Yield Curve (9303 days, 7 maturities)\n",
      "\n",
      "üéØ Total datasets: 7\n"
     ]
    }
   ],
   "source": [
    "def load_sp500_data():\n",
    "    \"\"\"Charge les donn√©es S&P 500 compl√®tes\"\"\"\n",
    "    log_returns = pd.read_csv(\"data/processed/dataset_log_returns.csv\", index_col=0, parse_dates=True)\n",
    "    log_returns = log_returns.dropna(how='all')\n",
    "    \n",
    "    # Pr√©traitement causal\n",
    "    X_df, W_df, M_df = preprocessing_dataset(\n",
    "        log_returns, \n",
    "        win=60, \n",
    "        min_periods=40,\n",
    "        clip_val=3.0,\n",
    "        min_valid_per_day=50  # Au moins 50 actions valides par jour\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'name': 'SP500_Full',\n",
    "        'X': X_df.values,\n",
    "        'dates': X_df.index,\n",
    "        'features': X_df.columns,\n",
    "        'description': f'S&P 500 Full ({X_df.shape[0]} days, {X_df.shape[1]} stocks)'\n",
    "    }\n",
    "\n",
    "def load_sectoral_data():\n",
    "    \"\"\"Charge les donn√©es sectorielles\"\"\"\n",
    "    sectors_data = {}\n",
    "    sectors_mapping = pd.read_csv(\"data/raw/tickers_sectors.csv\", index_col=0)\n",
    "    \n",
    "    # Secteurs principaux seulement (pour √©viter trop de datasets)\n",
    "    main_sectors = ['Information Technology', 'Financials', 'Health Care', \n",
    "                   'Industrials', 'Consumer Discretionary']\n",
    "    \n",
    "    for sector in main_sectors:\n",
    "        sector_path = f\"data/processed/sectors/{sector.lower().replace(' ', '_')}/log_returns.csv\"\n",
    "        if os.path.exists(sector_path):\n",
    "            sector_returns = pd.read_csv(sector_path, index_col=0, parse_dates=True)\n",
    "            sector_returns = sector_returns.dropna(how='all')\n",
    "            \n",
    "            # Pr√©traitement sectoriel\n",
    "            X_df, W_df, M_df = preprocessing_dataset(\n",
    "                sector_returns,\n",
    "                win=60,\n",
    "                min_periods=40,\n",
    "                clip_val=3.0,\n",
    "                min_valid_per_day=max(5, sector_returns.shape[1] // 3)\n",
    "            )\n",
    "            \n",
    "            sectors_data[sector] = {\n",
    "                'name': f'Sector_{sector.replace(\" \", \"_\")}',\n",
    "                'X': X_df.values,\n",
    "                'dates': X_df.index,\n",
    "                'features': X_df.columns,\n",
    "                'description': f'{sector} ({X_df.shape[0]} days, {X_df.shape[1]} stocks)'\n",
    "            }\n",
    "    \n",
    "    return sectors_data\n",
    "\n",
    "def load_yield_curve_data():\n",
    "    \"\"\"Charge et pr√©traite les donn√©es de courbe de taux\"\"\"\n",
    "    from src.utils.yield_curve_data import load_preprocessed_yield_curve\n",
    "    \n",
    "    # Charger les donn√©es avec normalisation cross-sectionnelle (privil√©gi√©e pour KAN AE)\n",
    "    df = load_preprocessed_yield_curve(\n",
    "        start=\"2000-01-01\",\n",
    "        normalization=\"cross_section\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'name': 'Yield_Curve',\n",
    "        'X': df.values,\n",
    "        'dates': df.index,\n",
    "        'features': df.columns,\n",
    "        'description': f'US Yield Curve ({df.shape[0]} days, {df.shape[1]} maturities)'\n",
    "    }\n",
    "\n",
    "# Chargement de tous les datasets\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "datasets = {}\n",
    "\n",
    "# S&P 500 complet\n",
    "datasets['SP500_Full'] = load_sp500_data()\n",
    "print(f\"‚úÖ {datasets['SP500_Full']['description']}\")\n",
    "\n",
    "# Donn√©es sectorielles\n",
    "sectoral_data = load_sectoral_data()\n",
    "datasets.update(sectoral_data)\n",
    "for sector_name, sector_data in sectoral_data.items():\n",
    "    print(f\"‚úÖ {sector_data['description']}\")\n",
    "\n",
    "# Courbe de taux\n",
    "datasets['Yield_Curve'] = load_yield_curve_data()\n",
    "print(f\"‚úÖ {datasets['Yield_Curve']['description']}\")\n",
    "\n",
    "print(f\"\\nüéØ Total datasets: {len(datasets)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9965e1d",
   "metadata": {},
   "source": [
    "## 4. Fonction d'Entra√Ænement Principal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8163df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_ae_config(X_data, config, dataset_name, cv_splits=3):\n",
    "    \"\"\"Entra√Æne un MLP AE avec une configuration donn√©e\"\"\"\n",
    "    results = {\n",
    "        'dataset': dataset_name,\n",
    "        'config': config,\n",
    "        'cv_scores': [],\n",
    "        'mean_score': 0,\n",
    "        'std_score': 0,\n",
    "        'training_time': 0,\n",
    "        'status': 'success'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Cross-validation temporelle\n",
    "        tscv = TimeSeriesSplit(n_splits=cv_splits)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_data)):\n",
    "            # Split des donn√©es\n",
    "            X_train, X_val = X_data[train_idx], X_data[val_idx]\n",
    "            \n",
    "            # Normalisation\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            \n",
    "            # Cr√©ation du mod√®le\n",
    "            model = MLPAutoencoder(\n",
    "                input_dim=X_train_scaled.shape[1],\n",
    "                k=config['latent_dims'],\n",
    "                hidden=config['hidden_layers'],\n",
    "                activation=config['activations'],\n",
    "                use_bn=config['use_batch_norm'],\n",
    "                dropout_p=config['dropout_rates'],\n",
    "                loss_type=config['loss_types']\n",
    "            )\n",
    "            \n",
    "            # Entra√Ænement\n",
    "            X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "            X_val_tensor = torch.FloatTensor(X_val_scaled)\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train_tensor,\n",
    "                # X_val=X_val_tensor,\n",
    "                epochs=config['epochs'],\n",
    "                batch_size=config['batch_sizes'],\n",
    "                learning_rate=config['learning_rates'],\n",
    "                weight_decay=config['weight_decays'],\n",
    "                patience=config['patience'],\n",
    "                verbose=False,  # Silencieux pour la grille\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # √âvaluation\n",
    "            performance = evaluate_model_performance(model, X_val_scaled)\n",
    "            cv_scores.append(performance['explained_variance'])\n",
    "            \n",
    "            # Nettoyage m√©moire\n",
    "            del model, X_train_tensor, X_val_tensor\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            gc.collect()\n",
    "        \n",
    "        # Agr√©gation des r√©sultats CV\n",
    "        results['cv_scores'] = cv_scores\n",
    "        results['mean_score'] = np.mean(cv_scores)\n",
    "        results['std_score'] = np.std(cv_scores)\n",
    "        results['training_time'] = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "    except Exception as e:\n",
    "        results['status'] = 'failed'\n",
    "        results['error'] = str(e)\n",
    "        results['mean_score'] = -1  # Score sentinelle pour √©checs\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff45754",
   "metadata": {},
   "source": [
    "## 5. Entra√Ænement Principal - Grille Compl√®te\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b56f778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Results directory: results\\mlp_ae_grid_search_20250902_034428\n",
      "üé≤ Sampled 20 configurations\n",
      "\n",
      "üöÄ Starting grid search: 140 total experiments\n",
      "‚è±Ô∏è  Estimated time: 4.7 hours\n",
      "\n",
      "\n",
      "üìä Processing SP500_Full: S&P 500 Full (3734 days, 423 stocks)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d1bf5b5b41421a8e0ee165a6ff83e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training SP500_Full:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SP500_Full: 14/20 successful\n",
      "   üèÜ Best score: 0.2957\n",
      "   ‚öôÔ∏è  Best config: latent=32, hidden=(128, 64), lr=0.0005\n",
      "\n",
      "üìä Processing Information Technology: Information Technology (3734 days, 53 stocks)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d523d54d474294820d2322ecae84da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Information Technology:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Information Technology: 14/20 successful\n",
      "   üèÜ Best score: 0.6372\n",
      "   ‚öôÔ∏è  Best config: latent=32, hidden=(128, 64), lr=0.0005\n",
      "\n",
      "üìä Processing Financials: Financials (3734 days, 64 stocks)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d344cdd5862e416495381467310d57e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Financials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Financials: 14/20 successful\n",
      "   üèÜ Best score: 0.6188\n",
      "   ‚öôÔ∏è  Best config: latent=32, hidden=(128, 64), lr=0.0005\n",
      "\n",
      "üìä Processing Health Care: Health Care (3734 days, 53 stocks)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd158d7f0b6b4246b1f08a61214c5aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Health Care:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Health Care: 14/20 successful\n",
      "   üèÜ Best score: 0.6470\n",
      "   ‚öôÔ∏è  Best config: latent=32, hidden=(128, 64), lr=0.0005\n",
      "\n",
      "üìä Processing Industrials: Industrials (3734 days, 64 stocks)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0a333d7ee8449388170665af5cd964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Industrials:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Industrials: 14/20 successful\n",
      "   üèÜ Best score: 0.5619\n",
      "   ‚öôÔ∏è  Best config: latent=32, hidden=(128, 64), lr=0.0005\n",
      "\n",
      "üìä Processing Consumer Discretionary: Consumer Discretionary (3734 days, 43 stocks)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a902e3584ec413399c977a7354f0744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Consumer Discretionary:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Consumer Discretionary: 14/20 successful\n",
      "   üèÜ Best score: 0.8057\n",
      "   ‚öôÔ∏è  Best config: latent=32, hidden=(128, 64), lr=0.0005\n",
      "\n",
      "üìä Processing Yield_Curve: US Yield Curve (9303 days, 7 maturities)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7569d7a28a9940bb9904b277934950b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Yield_Curve:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Yield_Curve: 20/20 successful\n",
      "   üèÜ Best score: 0.9650\n",
      "   ‚öôÔ∏è  Best config: latent=32, hidden=(128, 64), lr=0.0005\n",
      "\n",
      "üéâ Grid search completed! Results saved in: results\\mlp_ae_grid_search_20250902_034428\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation du r√©pertoire de r√©sultats\n",
    "results_dir = create_results_directory()\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "\n",
    "# √âchantillonnage des hyperparam√®tres\n",
    "sampled_configs = sample_hyperparams(HYPERPARAMS_GRID, MAX_COMBINATIONS_PER_DATASET)\n",
    "print(f\"üé≤ Sampled {len(sampled_configs)} configurations\")\n",
    "\n",
    "# Entra√Ænement sur tous les datasets\n",
    "all_results = {}\n",
    "total_experiments = len(datasets) * len(sampled_configs)\n",
    "experiment_count = 0\n",
    "\n",
    "print(f\"\\nüöÄ Starting grid search: {total_experiments} total experiments\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: {total_experiments * 2 / 60:.1f} hours\\n\")\n",
    "\n",
    "for dataset_name, dataset_info in datasets.items():\n",
    "    print(f\"\\nüìä Processing {dataset_name}: {dataset_info['description']}\")\n",
    "    \n",
    "    dataset_results = []\n",
    "    X_data = dataset_info['X']\n",
    "    \n",
    "    # Progress bar pour ce dataset\n",
    "    pbar = tqdm(sampled_configs, desc=f\"Training {dataset_name}\", leave=False)\n",
    "    \n",
    "    for config_idx, config in enumerate(pbar):\n",
    "        experiment_count += 1\n",
    "        \n",
    "        # Mise √† jour de la progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Exp': f\"{experiment_count}/{total_experiments}\",\n",
    "            'Config': f\"{config_idx+1}/{len(sampled_configs)}\"\n",
    "        })\n",
    "        \n",
    "        # Entra√Ænement\n",
    "        result = train_mlp_ae_config(X_data, config, dataset_name)\n",
    "        dataset_results.append(result)\n",
    "        \n",
    "        if (config_idx + 1) % 20 == 0:\n",
    "            temp_df = save_results(dataset_results, results_dir, f\"{dataset_name}_temp\")\n",
    "            best_score = temp_df['mean_score'].max()\n",
    "            pbar.set_postfix({\n",
    "                'Exp': f\"{experiment_count}/{total_experiments}\",\n",
    "                'Best': f\"{best_score:.4f}\"\n",
    "            })\n",
    "    \n",
    "    # Sauvegarde finale pour ce dataset\n",
    "    df_results = save_results(dataset_results, results_dir, dataset_name)\n",
    "    all_results[dataset_name] = dataset_results\n",
    "    \n",
    "    # R√©sum√© des performances\n",
    "    successful_runs = df_results[df_results['status'] == 'success']\n",
    "    if len(successful_runs) > 0:\n",
    "        best_score = successful_runs['mean_score'].max()\n",
    "        best_config_idx = successful_runs['mean_score'].idxmax()\n",
    "        best_config = successful_runs.loc[best_config_idx, 'config']\n",
    "        \n",
    "        print(f\"‚úÖ {dataset_name}: {len(successful_runs)}/{len(dataset_results)} successful\")\n",
    "        print(f\"   üèÜ Best score: {best_score:.4f}\")\n",
    "        print(f\"   ‚öôÔ∏è  Best config: latent={best_config['latent_dims']}, \"\n",
    "              f\"hidden={best_config['hidden_layers']}, lr={best_config['learning_rates']}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {dataset_name}: No successful runs\")\n",
    "    \n",
    "    # Nettoyage m√©moire\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nüéâ Grid search completed! Results saved in: {results_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3934dbd",
   "metadata": {},
   "source": [
    "## 6. Extraction et Analyse des Facteurs Latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5092840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Extracting latent factors from best models...\n",
      "üîç SP500_Full: Extracting factors (score: 0.2957)\n",
      "   ‚úÖ Extracted 32 factors, Explained Var: 0.3862\n",
      "üîç Information Technology: Extracting factors (score: 0.6372)\n",
      "   ‚úÖ Extracted 32 factors, Explained Var: 0.7514\n",
      "üîç Financials: Extracting factors (score: 0.6188)\n",
      "   ‚úÖ Extracted 32 factors, Explained Var: 0.7332\n",
      "üîç Health Care: Extracting factors (score: 0.6470)\n",
      "   ‚úÖ Extracted 32 factors, Explained Var: 0.7579\n",
      "üîç Industrials: Extracting factors (score: 0.5619)\n",
      "   ‚úÖ Extracted 32 factors, Explained Var: 0.6945\n",
      "üîç Consumer Discretionary: Extracting factors (score: 0.8057)\n",
      "   ‚úÖ Extracted 32 factors, Explained Var: 0.8851\n",
      "üîç Yield_Curve: Extracting factors (score: 0.9650)\n",
      "   ‚úÖ Extracted 32 factors, Explained Var: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def extract_latent_factors_best_models(results_dir, datasets):\n",
    "    \"\"\"Extrait les facteurs latents des meilleurs mod√®les pour chaque dataset\"\"\"\n",
    "    \n",
    "    print(\"üß† Extracting latent factors from best models...\")\n",
    "    \n",
    "    # Chargement des r√©sultats\n",
    "    all_results = []\n",
    "    for csv_file in results_dir.glob(\"*_results.csv\"):\n",
    "        if \"temp\" not in csv_file.name:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            df['dataset'] = csv_file.stem.replace('_results', '')\n",
    "            all_results.append(df)\n",
    "    \n",
    "    if not all_results:\n",
    "        print(\"‚ùå No results found\")\n",
    "        return {}\n",
    "    \n",
    "    combined_df = pd.concat(all_results, ignore_index=True)\n",
    "    successful_df = combined_df[combined_df['status'] == 'success']\n",
    "    \n",
    "    latent_factors = {}\n",
    "    \n",
    "    for dataset_name in datasets.keys():\n",
    "        dataset_results = successful_df[successful_df['dataset'] == dataset_name]\n",
    "        \n",
    "        if len(dataset_results) == 0:\n",
    "            print(f\"‚ö†Ô∏è  No successful results for {dataset_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Meilleure configuration\n",
    "        best_result = dataset_results.loc[dataset_results['mean_score'].idxmax()]\n",
    "        best_config = eval(best_result['config'])\n",
    "        \n",
    "        print(f\"üîç {dataset_name}: Extracting factors (score: {best_result['mean_score']:.4f})\")\n",
    "        \n",
    "        # Donn√©es du dataset\n",
    "        X_data = datasets[dataset_name]['X']\n",
    "        \n",
    "        # Normalisation\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_data)\n",
    "        \n",
    "        # Entra√Ænement du meilleur mod√®le sur tout le dataset\n",
    "        model = MLPAutoencoder(\n",
    "            input_dim=X_scaled.shape[1],\n",
    "            k=best_config['latent_dims'],\n",
    "            hidden=best_config['hidden_layers'],\n",
    "            activation=best_config['activations'],\n",
    "            use_bn=best_config['use_batch_norm'],\n",
    "            dropout_p=best_config['dropout_rates'],\n",
    "            loss_type=best_config['loss_types']\n",
    "        )\n",
    "        \n",
    "        # Entra√Ænement final (80/20 split)\n",
    "        split_idx = int(0.8 * len(X_scaled))\n",
    "        X_train_final = torch.FloatTensor(X_scaled[:split_idx])\n",
    "        X_val_final = torch.FloatTensor(X_scaled[split_idx:])\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train_final,\n",
    "            # X_val=X_val_final,\n",
    "            epochs=best_config['epochs'],\n",
    "            batch_size=best_config['batch_sizes'],\n",
    "            learning_rate=best_config['learning_rates'],\n",
    "            weight_decay=best_config['weight_decays'],\n",
    "            patience=best_config['patience'],\n",
    "            verbose=False,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Extraction des facteurs latents\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "            X_recon, latent_factors_tensor = model(X_tensor)\n",
    "            \n",
    "            latent_np = latent_factors_tensor.cpu().numpy()\n",
    "            recon_np = X_recon.cpu().numpy()\n",
    "        \n",
    "        # Calcul des m√©triques de performance\n",
    "        mse = np.mean((X_scaled - recon_np) ** 2)\n",
    "        explained_var = 1 - np.var(X_scaled - recon_np) / np.var(X_scaled)\n",
    "        \n",
    "        # Stockage des r√©sultats\n",
    "        latent_factors[dataset_name] = {\n",
    "            'factors': latent_np,\n",
    "            'reconstruction': recon_np,\n",
    "            'original': X_scaled,\n",
    "            'dates': datasets[dataset_name]['dates'],\n",
    "            'features': datasets[dataset_name]['features'],\n",
    "            'config': best_config,\n",
    "            'performance': {\n",
    "                'mse': mse,\n",
    "                'explained_variance': explained_var,\n",
    "                'cv_score': best_result['mean_score']\n",
    "            },\n",
    "            'scaler': scaler\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Extracted {latent_np.shape[1]} factors, Explained Var: {explained_var:.4f}\")\n",
    "        \n",
    "        # Nettoyage m√©moire\n",
    "        del model, X_tensor, latent_factors_tensor, X_recon\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "    \n",
    "    return latent_factors\n",
    "\n",
    "# Extraction des facteurs latents\n",
    "latent_factors_mlp = extract_latent_factors_best_models(results_dir, datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d251c",
   "metadata": {},
   "source": [
    "## 7. Comparaison avec PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bc333f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Performing PCA comparison...\n",
      "\n",
      "üîç Analyzing SP500_Full\n",
      "   üìà PCA Explained Var: 0.4056\n",
      "   ü§ñ MLP Explained Var: 0.3862\n",
      "   üìä Improvement: -0.0194\n",
      "   üîó Mean Factor Correlation: 0.4318\n",
      "\n",
      "üîç Analyzing Information Technology\n",
      "   üìà PCA Explained Var: 0.7600\n",
      "   ü§ñ MLP Explained Var: 0.7514\n",
      "   üìä Improvement: -0.0086\n",
      "   üîó Mean Factor Correlation: 0.4276\n",
      "\n",
      "üîç Analyzing Financials\n",
      "   üìà PCA Explained Var: 0.7444\n",
      "   ü§ñ MLP Explained Var: 0.7332\n",
      "   üìä Improvement: -0.0111\n",
      "   üîó Mean Factor Correlation: 0.4134\n",
      "\n",
      "üîç Analyzing Health Care\n",
      "   üìà PCA Explained Var: 0.7709\n",
      "   ü§ñ MLP Explained Var: 0.7579\n",
      "   üìä Improvement: -0.0130\n",
      "   üîó Mean Factor Correlation: 0.4206\n",
      "\n",
      "üîç Analyzing Industrials\n",
      "   üìà PCA Explained Var: 0.7053\n",
      "   ü§ñ MLP Explained Var: 0.6945\n",
      "   üìä Improvement: -0.0108\n",
      "   üîó Mean Factor Correlation: 0.4036\n",
      "\n",
      "üîç Analyzing Consumer Discretionary\n",
      "   üìà PCA Explained Var: 0.8921\n",
      "   ü§ñ MLP Explained Var: 0.8851\n",
      "   üìä Improvement: -0.0070\n",
      "   üîó Mean Factor Correlation: 0.4210\n",
      "\n",
      "üîç Analyzing Yield_Curve\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=32 must be between 0 and min(n_samples, n_features)=7 with svd_solver='covariance_eigh'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pca_results, comparison_results\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Comparaison avec PCA\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m pca_results, comparison_results \u001b[38;5;241m=\u001b[39m \u001b[43mperform_pca_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_factors_mlp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m, in \u001b[0;36mperform_pca_comparison\u001b[1;34m(datasets, latent_factors_mlp)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# PCA avec m√™me nombre de composantes que MLP AE\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39mn_components)\n\u001b[1;32m---> 26\u001b[0m pca_factors \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m pca_reconstruction \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39minverse_transform(pca_factors)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# M√©triques PCA\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\decomposition\\_pca.py:468\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    447\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 468\u001b[0m     U, S, _, X, x_is_centered, xp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\decomposition\\_pca.py:542\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcovariance_eigh\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandomized\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_truncated(X, n_components, xp)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\decomposition\\_pca.py:556\u001b[0m, in \u001b[0;36mPCA._fit_full\u001b[1;34m(self, X, n_components, xp, is_array_api_compliant)\u001b[0m\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmle\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is only supported if n_samples >= n_features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_components \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    557\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be between 0 and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    558\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    560\u001b[0m     )\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mmean(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components=32 must be between 0 and min(n_samples, n_features)=7 with svd_solver='covariance_eigh'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def perform_pca_comparison(datasets, latent_factors_mlp):\n",
    "    \"\"\"Compare MLP AE factors with PCA\"\"\"\n",
    "    \n",
    "    print(\"üìä Performing PCA comparison...\")\n",
    "    \n",
    "    pca_results = {}\n",
    "    comparison_results = {}\n",
    "    \n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        if dataset_name not in latent_factors_mlp:\n",
    "            print(f\"‚ö†Ô∏è  Skipping {dataset_name} - no MLP results\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüîç Analyzing {dataset_name}\")\n",
    "        \n",
    "        # Donn√©es normalis√©es (m√™me que MLP AE)\n",
    "        X_scaled = latent_factors_mlp[dataset_name]['original']\n",
    "        n_components = latent_factors_mlp[dataset_name]['factors'].shape[1]\n",
    "        \n",
    "        # PCA avec m√™me nombre de composantes que MLP AE\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_factors = pca.fit_transform(X_scaled)\n",
    "        pca_reconstruction = pca.inverse_transform(pca_factors)\n",
    "        \n",
    "        # M√©triques PCA\n",
    "        pca_mse = mean_squared_error(X_scaled, pca_reconstruction)\n",
    "        pca_explained_var = 1 - np.var(X_scaled - pca_reconstruction) / np.var(X_scaled)\n",
    "        \n",
    "        pca_results[dataset_name] = {\n",
    "            'factors': pca_factors,\n",
    "            'reconstruction': pca_reconstruction,\n",
    "            'components': pca.components_,\n",
    "            'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "            'performance': {\n",
    "                'mse': pca_mse,\n",
    "                'explained_variance': pca_explained_var,\n",
    "                'cumulative_variance': np.cumsum(pca.explained_variance_ratio_)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Comparaison MLP AE vs PCA\n",
    "        mlp_factors = latent_factors_mlp[dataset_name]['factors']\n",
    "        mlp_performance = latent_factors_mlp[dataset_name]['performance']\n",
    "        \n",
    "        # Corr√©lations entre facteurs (ordre peut √™tre diff√©rent)\n",
    "        factor_correlations = []\n",
    "        for i in range(n_components):\n",
    "            max_corr = 0\n",
    "            best_j = 0\n",
    "            for j in range(n_components):\n",
    "                corr, _ = pearsonr(mlp_factors[:, i], pca_factors[:, j])\n",
    "                if abs(corr) > abs(max_corr):\n",
    "                    max_corr = corr\n",
    "                    best_j = j\n",
    "            factor_correlations.append((i, best_j, max_corr))\n",
    "        \n",
    "        # Comparaison des performances\n",
    "        performance_comparison = {\n",
    "            'mlp_explained_var': mlp_performance['explained_variance'],\n",
    "            'pca_explained_var': pca_explained_var,\n",
    "            'mlp_mse': mlp_performance['mse'],\n",
    "            'pca_mse': pca_mse,\n",
    "            'improvement': mlp_performance['explained_variance'] - pca_explained_var,\n",
    "            'factor_correlations': factor_correlations,\n",
    "            'max_correlation': max([abs(corr) for _, _, corr in factor_correlations]),\n",
    "            'mean_correlation': np.mean([abs(corr) for _, _, corr in factor_correlations])\n",
    "        }\n",
    "        \n",
    "        comparison_results[dataset_name] = performance_comparison\n",
    "        \n",
    "        print(f\"   üìà PCA Explained Var: {pca_explained_var:.4f}\")\n",
    "        print(f\"   ü§ñ MLP Explained Var: {mlp_performance['explained_variance']:.4f}\")\n",
    "        print(f\"   üìä Improvement: {performance_comparison['improvement']:.4f}\")\n",
    "        print(f\"   üîó Mean Factor Correlation: {performance_comparison['mean_correlation']:.4f}\")\n",
    "        \n",
    "    return pca_results, comparison_results\n",
    "\n",
    "# Comparaison avec PCA\n",
    "pca_results, comparison_results = perform_pca_comparison(datasets, latent_factors_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd2b49",
   "metadata": {},
   "source": [
    "## 8. Visualisations Comparatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4593eeb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comparison_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 198\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Cr√©ation des visualisations comparatives\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcomparison_results\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m pca_results:\n\u001b[0;32m    199\u001b[0m     create_comprehensive_comparison_plots(comparison_results, pca_results, latent_factors_mlp, results_dir)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comparison_results' is not defined"
     ]
    }
   ],
   "source": [
    "def create_comprehensive_comparison_plots(comparison_results, pca_results, latent_factors_mlp, results_dir):\n",
    "    \"\"\"Cr√©e des visualisations comparatives compl√®tes MLP AE vs PCA\"\"\"\n",
    "    \n",
    "    # Configuration des couleurs selon les pr√©f√©rences\n",
    "    colors = {\n",
    "        'mlp': '#FF6B6B',      # Rouge\n",
    "        'pca': '#4ECDC4',      # Vert/Teal\n",
    "        'improvement': '#45B7D1', # Bleu\n",
    "        'correlation': '#FECA57'  # Jaune/Orange\n",
    "    }\n",
    "    \n",
    "    # Cr√©ation de la figure principale\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # Donn√©es pour les plots\n",
    "    datasets_with_results = list(comparison_results.keys())\n",
    "    \n",
    "    if not datasets_with_results:\n",
    "        print(\"‚ùå No comparison data available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # 1. Comparaison des performances globales\n",
    "    ax1 = fig.add_subplot(gs[0, :2])\n",
    "    \n",
    "    mlp_scores = [comparison_results[ds]['mlp_explained_var'] for ds in datasets_with_results]\n",
    "    pca_scores = [comparison_results[ds]['pca_explained_var'] for ds in datasets_with_results]\n",
    "    \n",
    "    x_pos = np.arange(len(datasets_with_results))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x_pos - width/2, mlp_scores, width, label='MLP AE', color=colors['mlp'], alpha=0.8)\n",
    "    bars2 = ax1.bar(x_pos + width/2, pca_scores, width, label='PCA', color=colors['pca'], alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Dataset')\n",
    "    ax1.set_ylabel('Explained Variance')\n",
    "    ax1.set_title('MLP Autoencoder vs PCA: Explained Variance Comparison', fontweight='bold', fontsize=14)\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels([ds.replace('_', '\\n') for ds in datasets_with_results], rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajout des valeurs sur les barres\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Am√©lioration MLP AE vs PCA\n",
    "    ax2 = fig.add_subplot(gs[0, 2:])\n",
    "    \n",
    "    improvements = [comparison_results[ds]['improvement'] for ds in datasets_with_results]\n",
    "    colors_improvement = [colors['improvement'] if imp >= 0 else colors['mlp'] for imp in improvements]\n",
    "    \n",
    "    bars = ax2.bar(x_pos, improvements, color=colors_improvement, alpha=0.8)\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax2.set_xlabel('Dataset')\n",
    "    ax2.set_ylabel('Improvement (MLP AE - PCA)')\n",
    "    ax2.set_title('Performance Improvement: MLP AE over PCA', fontweight='bold', fontsize=14)\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels([ds.replace('_', '\\n') for ds in datasets_with_results], rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ajout des valeurs\n",
    "    for bar, imp in zip(bars, improvements):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + (0.002 if height >= 0 else -0.005),\n",
    "                f'{imp:+.4f}', ha='center', va='bottom' if height >= 0 else 'top', fontweight='bold')\n",
    "    \n",
    "    # 3. Corr√©lations entre facteurs\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    \n",
    "    mean_correlations = [comparison_results[ds]['mean_correlation'] for ds in datasets_with_results]\n",
    "    bars = ax3.bar(x_pos, mean_correlations, color=colors['correlation'], alpha=0.8)\n",
    "    ax3.set_xlabel('Dataset')\n",
    "    ax3.set_ylabel('Mean |Correlation|')\n",
    "    ax3.set_title('Factor Correlations\\n(MLP AE vs PCA)', fontweight='bold', fontsize=12)\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels([ds.replace('_', '\\n') for ds in datasets_with_results], rotation=45, ha='right')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Distribution des am√©liorations\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    ax4.hist(improvements, bins=10, color=colors['improvement'], alpha=0.7, edgecolor='black')\n",
    "    ax4.axvline(x=0, color='red', linestyle='--', alpha=0.8, label='No improvement')\n",
    "    ax4.axvline(x=np.mean(improvements), color='green', linestyle='-', alpha=0.8, label=f'Mean: {np.mean(improvements):.4f}')\n",
    "    ax4.set_xlabel('Improvement (Explained Variance)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Distribution of\\nPerformance Improvements', fontweight='bold', fontsize=12)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Scatter plot: MLP AE vs PCA performance\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    \n",
    "    scatter = ax5.scatter(pca_scores, mlp_scores, c=mean_correlations, cmap='viridis', \n",
    "                         s=100, alpha=0.8, edgecolors='black', linewidth=1)\n",
    "    \n",
    "    # Ligne de parit√©\n",
    "    min_score = min(min(pca_scores), min(mlp_scores))\n",
    "    max_score = max(max(pca_scores), max(mlp_scores))\n",
    "    ax5.plot([min_score, max_score], [min_score, max_score], 'r--', alpha=0.8, label='Parity line')\n",
    "    \n",
    "    ax5.set_xlabel('PCA Explained Variance')\n",
    "    ax5.set_ylabel('MLP AE Explained Variance')\n",
    "    ax5.set_title('Performance Scatter\\n(colored by correlation)', fontweight='bold', fontsize=12)\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax5, shrink=0.8)\n",
    "    cbar.set_label('Mean Factor Correlation')\n",
    "    \n",
    "    # 6. Nombre de facteurs latents par dataset\n",
    "    ax6 = fig.add_subplot(gs[1, 3])\n",
    "    \n",
    "    n_factors = [latent_factors_mlp[ds]['factors'].shape[1] for ds in datasets_with_results]\n",
    "    bars = ax6.bar(x_pos, n_factors, color=colors['mlp'], alpha=0.8)\n",
    "    ax6.set_xlabel('Dataset')\n",
    "    ax6.set_ylabel('Number of Latent Factors')\n",
    "    ax6.set_title('Latent Dimensions\\nper Dataset', fontweight='bold', fontsize=12)\n",
    "    ax6.set_xticks(x_pos)\n",
    "    ax6.set_xticklabels([ds.replace('_', '\\n') for ds in datasets_with_results], rotation=45, ha='right')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 7-10. √âvolution temporelle des facteurs pour datasets s√©lectionn√©s\n",
    "    selected_datasets = list(datasets_with_results)[:4]  # 4 premiers datasets\n",
    "    \n",
    "    for idx, dataset_name in enumerate(selected_datasets):\n",
    "        ax = fig.add_subplot(gs[2 + idx//2, (idx%2)*2:(idx%2)*2+2])\n",
    "        \n",
    "        # Facteurs MLP AE\n",
    "        mlp_factors = latent_factors_mlp[dataset_name]['factors']\n",
    "        dates = latent_factors_mlp[dataset_name]['dates']\n",
    "        \n",
    "        # Facteurs PCA\n",
    "        pca_factors = pca_results[dataset_name]['factors']\n",
    "        \n",
    "        # Plot des 3 premiers facteurs de chaque m√©thode\n",
    "        n_factors_to_plot = min(3, mlp_factors.shape[1])\n",
    "        \n",
    "        for i in range(n_factors_to_plot):\n",
    "            ax.plot(dates[:len(mlp_factors)], mlp_factors[:, i], \n",
    "                   label=f'MLP F{i+1}', color=colors['mlp'], alpha=0.7, linewidth=1.5)\n",
    "            ax.plot(dates[:len(pca_factors)], pca_factors[:, i], \n",
    "                   label=f'PCA F{i+1}', color=colors['pca'], alpha=0.7, linestyle='--', linewidth=1)\n",
    "        \n",
    "        ax.set_title(f'{dataset_name.replace(\"_\", \" \")} - Factor Evolution', fontweight='bold', fontsize=11)\n",
    "        ax.set_xlabel('Time')\n",
    "        ax.set_ylabel('Factor Value')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Rotation des dates\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Titre principal\n",
    "    fig.suptitle('MLP Autoencoder vs PCA: Comprehensive Comparison Analysis', \n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Sauvegarde\n",
    "    viz_file = results_dir / \"mlp_ae_vs_pca_comprehensive_analysis.png\"\n",
    "    plt.savefig(viz_file, dpi=300, bbox_inches='tight', facecolor='white', edgecolor='none')\n",
    "    print(f\"üìä Comprehensive visualization saved to: {viz_file}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # R√©sum√© statistique\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìà MLP AUTOENCODER vs PCA - STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä Datasets analyzed: {len(datasets_with_results)}\")\n",
    "    print(f\"üèÜ MLP AE wins: {sum(1 for imp in improvements if imp > 0)} / {len(improvements)}\")\n",
    "    print(f\"üìà Average improvement: {np.mean(improvements):.4f} ¬± {np.std(improvements):.4f}\")\n",
    "    print(f\"üîó Average factor correlation: {np.mean(mean_correlations):.4f} ¬± {np.std(mean_correlations):.4f}\")\n",
    "    print(f\"üìä Best improvement: {max(improvements):.4f} ({datasets_with_results[improvements.index(max(improvements))]})\")\n",
    "    print(f\"üìä Worst performance: {min(improvements):.4f} ({datasets_with_results[improvements.index(min(improvements))]})\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Cr√©ation des visualisations comparatives\n",
    "if comparison_results and pca_results:\n",
    "    create_comprehensive_comparison_plots(comparison_results, pca_results, latent_factors_mlp, results_dir)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No comparison data available for visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392da14",
   "metadata": {},
   "source": [
    "## 9. Sauvegarde des R√©sultats Finaux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c45470",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pca_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sauvegarde compl√®te des r√©sultats pour comparaison future avec KAN AE\u001b[39;00m\n\u001b[0;32m      2\u001b[0m final_results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp_ae_factors\u001b[39m\u001b[38;5;124m'\u001b[39m: latent_factors_mlp,\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpca_results\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mpca_results\u001b[49m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomparison_results\u001b[39m\u001b[38;5;124m'\u001b[39m: comparison_results,\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets_info\u001b[39m\u001b[38;5;124m'\u001b[39m: {name: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m: info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m: info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]} \n\u001b[0;32m      7\u001b[0m                      \u001b[38;5;28;01mfor\u001b[39;00m name, info \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat(),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparams_tested\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(sampled_configs),\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_experiments\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(datasets) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(sampled_configs)\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Sauvegarde JSON pour m√©tadonn√©es\u001b[39;00m\n\u001b[0;32m     14\u001b[0m final_results_file \u001b[38;5;241m=\u001b[39m results_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlp_ae_final_results.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pca_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Sauvegarde compl√®te des r√©sultats pour comparaison future avec KAN AE\n",
    "final_results = {\n",
    "    'mlp_ae_factors': latent_factors_mlp,\n",
    "    'pca_results': pca_results,\n",
    "    'comparison_results': comparison_results,\n",
    "    'datasets_info': {name: {'shape': info['X'].shape, 'description': info['description']} \n",
    "                     for name, info in datasets.items()},\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'hyperparams_tested': len(sampled_configs),\n",
    "    'total_experiments': len(datasets) * len(sampled_configs)\n",
    "}\n",
    "\n",
    "# Sauvegarde JSON pour m√©tadonn√©es\n",
    "final_results_file = results_dir / \"mlp_ae_final_results.json\"\n",
    "with open(final_results_file, 'w') as f:\n",
    "    # Conversion des arrays numpy en listes pour JSON\n",
    "    json_safe_results = {}\n",
    "    for key, value in final_results.items():\n",
    "        if key in ['mlp_ae_factors', 'pca_results']:\n",
    "            json_safe_results[key] = {\n",
    "                dataset: {\n",
    "                    'performance': data.get('performance', {}),\n",
    "                    'config': data.get('config', {}),\n",
    "                    'factors_shape': data.get('factors', np.array([])).shape,\n",
    "                    'dates_count': len(data.get('dates', []))\n",
    "                }\n",
    "                for dataset, data in value.items()\n",
    "            }\n",
    "        else:\n",
    "            json_safe_results[key] = value\n",
    "    \n",
    "    json.dump(json_safe_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üíæ Final results saved to: {final_results_file}\")\n",
    "\n",
    "# R√©sum√© final\n",
    "print(f\"\\nüéâ MLP AUTOENCODER ANALYSIS COMPLETED!\")\n",
    "print(f\"=\"*60)\n",
    "print(f\"üìä Total datasets processed: {len(datasets)}\")\n",
    "print(f\"üß† Latent factors extracted: {len(latent_factors_mlp)}\")\n",
    "print(f\"üìà PCA comparisons: {len(pca_results)}\")\n",
    "print(f\"‚öôÔ∏è  Hyperparameters tested: {len(sampled_configs)}\")\n",
    "print(f\"üìÅ Results directory: {results_dir}\")\n",
    "print(f\"‚è∞ Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"=\"*60)\n",
    "\n",
    "if comparison_results:\n",
    "    improvements = [comparison_results[ds]['improvement'] for ds in comparison_results.keys()]\n",
    "    print(f\"üèÜ MLP AE outperforms PCA in {sum(1 for imp in improvements if imp > 0)}/{len(improvements)} datasets\")\n",
    "    print(f\"üìä Average improvement: {np.mean(improvements):.4f}\")\n",
    "    print(f\"üîù Best performing dataset: {list(comparison_results.keys())[improvements.index(max(improvements))]}\")\n",
    "    \n",
    "print(f\"\\n‚úÖ Ready for KAN AE comparison!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286247de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e34411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
